{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7c052d9256b4ed2b6c5697b6b79db4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_739da49d68d940b09f1c2b54d421625b",
              "IPY_MODEL_b23420f863d6458bb4fd72a6599855cd",
              "IPY_MODEL_80d0b69baab3404aa6d1e3fb9b905200",
              "IPY_MODEL_46eb3bf96d2d41a5931ec2510b41fb55",
              "IPY_MODEL_21d00e7fcac54679823cbd9f093322fc"
            ],
            "layout": "IPY_MODEL_6097fbf371a5489bab1cd896751e929b"
          }
        },
        "739da49d68d940b09f1c2b54d421625b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec93040b73694b5b9a3648f574f14f7b",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf2e34cdce4436fa1a12f85c3a318c4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b23420f863d6458bb4fd72a6599855cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_acda48160e7e4a6a87396e72f815084c",
            "placeholder": "​",
            "style": "IPY_MODEL_109b4920e05e4b42ad52191a95b3db27",
            "value": ""
          }
        },
        "80d0b69baab3404aa6d1e3fb9b905200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_f15895639044492f83e909eb5bd3db3b",
            "style": "IPY_MODEL_89d7ccda91e943b4b3f87e250adb49f5",
            "value": true
          }
        },
        "46eb3bf96d2d41a5931ec2510b41fb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_979d68030fb440d081bac209f62e73d8",
            "style": "IPY_MODEL_2899b16b53004e91a05456ccca5be7be",
            "tooltip": ""
          }
        },
        "21d00e7fcac54679823cbd9f093322fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a5e4552d81041dc89b996aed3a482ca",
            "placeholder": "​",
            "style": "IPY_MODEL_48e5474ca9b248cca813375cb5740d62",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "6097fbf371a5489bab1cd896751e929b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ec93040b73694b5b9a3648f574f14f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf2e34cdce4436fa1a12f85c3a318c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acda48160e7e4a6a87396e72f815084c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109b4920e05e4b42ad52191a95b3db27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f15895639044492f83e909eb5bd3db3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d7ccda91e943b4b3f87e250adb49f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "979d68030fb440d081bac209f62e73d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2899b16b53004e91a05456ccca5be7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8a5e4552d81041dc89b996aed3a482ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e5474ca9b248cca813375cb5740d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf KoreanStandardPronunciation\n",
        "!git clone https://github.com/dhkang01/KoreanStandardPronunciation.git\n",
        "%cd KoreanStandardPronunciation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQXr7aQYqQ-d",
        "outputId": "500f1772-991c-4fc4-8c81-d9e4a207fb6b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'KoreanStandardPronunciation'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 46 (delta 23), reused 17 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (46/46), 120.03 KiB | 1.07 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "/content/KoreanStandardPronunciation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/models/kocharelectra-pron-lora-adapter\"\n",
        "\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCbLAYFmsJCb",
        "outputId": "42e4fe76-4a11-418e-8cd5-90899601fa73"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers>=4.38.0\" \"datasets>=2.18.0\" \"peft>=0.11.0\" accelerate huggingface_hub evaluate"
      ],
      "metadata": {
        "id": "R7c-gPQIWyx3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "a7c052d9256b4ed2b6c5697b6b79db4b",
            "739da49d68d940b09f1c2b54d421625b",
            "b23420f863d6458bb4fd72a6599855cd",
            "80d0b69baab3404aa6d1e3fb9b905200",
            "46eb3bf96d2d41a5931ec2510b41fb55",
            "21d00e7fcac54679823cbd9f093322fc",
            "6097fbf371a5489bab1cd896751e929b",
            "ec93040b73694b5b9a3648f574f14f7b",
            "3cf2e34cdce4436fa1a12f85c3a318c4",
            "acda48160e7e4a6a87396e72f815084c",
            "109b4920e05e4b42ad52191a95b3db27",
            "f15895639044492f83e909eb5bd3db3b",
            "89d7ccda91e943b4b3f87e250adb49f5",
            "979d68030fb440d081bac209f62e73d8",
            "2899b16b53004e91a05456ccca5be7be",
            "8a5e4552d81041dc89b996aed3a482ca",
            "48e5474ca9b248cca813375cb5740d62"
          ]
        },
        "id": "UH7gLzX7SvVo",
        "outputId": "ea233b72-2d4f-464b-8280-dc945bb3e1af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7c052d9256b4ed2b6c5697b6b79db4b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"dhkang01/KMA_dataset\"\n",
        "raw_ds = load_dataset(dataset_id, split=\"train\")\n",
        "\n",
        "raw_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnRX4nxUW1om",
        "outputId": "3afc7eec-0f88-48a8-a69f-bd418df4378b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'input', 'output'],\n",
              "    num_rows: 447115\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_tmp = raw_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = ds_train_tmp[\"train\"]\n",
        "tmp_ds   = ds_train_tmp[\"test\"]\n",
        "\n",
        "ds_val_test = tmp_ds.train_test_split(test_size=0.5, seed=42)\n",
        "val_ds = ds_val_test[\"train\"]\n",
        "test_ds = ds_val_test[\"test\"]\n",
        "\n",
        "# train/val/test -> 90/5/5\n",
        "\n",
        "train_ds = train_ds.select(range(1000))\n",
        "val_ds = val_ds.select(range(500))\n",
        "test_ds = test_ds.select(range(10))\n"
      ],
      "metadata": {
        "id": "pHHd_N49W40V"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer 다운로드"
      ],
      "metadata": {
        "id": "WFgn1ry0XLLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from KoCharELECTRA.tokenization_kocharelectra import KoCharElectraTokenizer\n",
        "\n",
        "model_name = \"monologg/kocharelectra-small-discriminator\"\n",
        "\n",
        "tokenizer = KoCharElectraTokenizer.from_pretrained(model_name)\n",
        "print(tokenizer.tokenize(\"가나다\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9JuUUxXW6Ke",
        "outputId": "826f3460-14b0-41cd-a5c5-66968b5bdb2b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
            "The class this function is called from is 'KoCharElectraTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['가', '나', '다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output vocab"
      ],
      "metadata": {
        "id": "mSW8hqgWZCeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# tokenizer.vocab은 OrderedDict(토큰 → ID)\n",
        "token_list = list(tokenizer.vocab.keys())\n",
        "\n",
        "pron2id = OrderedDict()\n",
        "for idx, tok in enumerate(token_list):\n",
        "    pron2id[tok] = idx\n",
        "\n",
        "id2pron = {v: k for k, v in pron2id.items()}\n",
        "\n",
        "len(pron2id), list(list(pron2id.items())[:10])"
      ],
      "metadata": {
        "id": "Ac7KYkFwW9xP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832249c0-c272-4495-ef61-64ad2879ac51"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11568,\n",
              " [('[PAD]', 0),\n",
              "  ('[UNK]', 1),\n",
              "  ('[CLS]', 2),\n",
              "  ('[SEP]', 3),\n",
              "  ('[MASK]', 4),\n",
              "  (' ', 5),\n",
              "  ('이', 6),\n",
              "  ('다', 7),\n",
              "  ('는', 8),\n",
              "  ('에', 9)])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리 함수 정의 및 적용\n",
        "\n",
        "복수 발음 허용 X"
      ],
      "metadata": {
        "id": "C_BE4tNIMB8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "max_length = 128  # 필요에 따라 조절\n",
        "\n",
        "def preprocess_example(example):\n",
        "    text = example[\"input\"]\n",
        "    pron = example[\"output\"]  # List[List[str]]\n",
        "\n",
        "    # KoCharElectra는 char 단위 토큰 + [CLS], [SEP]\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",  # DataCollator 써도 되지만 여기서는 고정 길이로\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    attention_mask = encoding[\"attention_mask\"]\n",
        "\n",
        "    # Electra/KoCharElectra: 대체로 [CLS] + chars + [SEP]\n",
        "    # => 실제 문자 수 = len(text)\n",
        "    # => pron 길이와 len(text)가 맞는다고 가정 (안 맞는 샘플은 나중에 필터 가능)\n",
        "    seq_len = sum(attention_mask)  # 실제 non-pad 길이\n",
        "    # [CLS] at 0, [SEP] at seq_len-1, chars in 1..seq_len-2\n",
        "\n",
        "    labels = np.full_like(input_ids, fill_value=-100)  # default ignore_index\n",
        "\n",
        "    # 문자 수와 pron 길이 안 맞으면 그냥 전부 ignore(-100)로 두고 스킵되게 할 수도 있음\n",
        "    # 여기선 일단 최소한으로만 체크\n",
        "    n_chars = seq_len - 2  # CLS, SEP 제외\n",
        "\n",
        "    if len(pron) != n_chars:\n",
        "        # 불일치하는 경우: 전부 padding label로 두고, 나중에 이런 샘플 비율 보고 판단\n",
        "        print(f\"Warning: pron len {len(pron)} != n_chars {n_chars} for text: {text}\")\n",
        "        print(f\"in the case, pron: {\"\".join([l[0] for l in pron])}\")\n",
        "        encoding[\"labels\"] = labels.tolist()\n",
        "        return encoding\n",
        "\n",
        "    for i in range(len(pron)):\n",
        "        cand_list = pron[i]\n",
        "        if not cand_list:\n",
        "            continue\n",
        "        label_id = pron2id[cand_list[0]]               # 첫 후보를 gold label로 사용\n",
        "        if label_id < 6:                               # 특수토큰, 띄어쓰기는 사용 X\n",
        "            continue\n",
        "\n",
        "        token_pos = 1 + i                      # 0: [CLS], 1.. : chars\n",
        "        if token_pos < seq_len - 1:            # 마지막 [SEP] 전까지만\n",
        "            labels[token_pos] = label_id\n",
        "\n",
        "    encoding[\"labels\"] = labels.tolist()\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "WL0yWezbW_J5"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized = train_ds.map(\n",
        "    preprocess_example,\n",
        "    remove_columns=train_ds.column_names,\n",
        ")\n",
        "\n",
        "val_tokenized = val_ds.map(\n",
        "    preprocess_example,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")\n",
        "\n",
        "test_tokenized = test_ds.map(\n",
        "    preprocess_example,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")\n",
        "\n",
        "# too long seq is out.\n",
        "\n",
        "# train_tokenized[0]\n"
      ],
      "metadata": {
        "id": "fWYWScxeXBJD",
        "collapsed": true
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 로드, LoRA 적용\n",
        "\n",
        "encoder에 LoRA적용\n",
        "classifier에 LoRA적용X, 전부 trainable"
      ],
      "metadata": {
        "id": "AQJtsdUuZsz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "char_embed 설정\n",
        "\n",
        "char->vec(41dim)\n",
        "\n",
        "emb wrapping"
      ],
      "metadata": {
        "id": "c7YXVgAfekl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14개 기본 자음 (쌍자음은 여기에 포함 X)\n",
        "BASE_CONSONANTS = [\"ㄱ\",\"ㄴ\",\"ㄷ\",\"ㄹ\",\"ㅁ\",\"ㅂ\",\"ㅅ\",\"ㅇ\",\"ㅈ\",\"ㅊ\",\"ㅋ\",\"ㅌ\",\"ㅍ\",\"ㅎ\"]\n",
        "CONSONANT2IDX = {c: i for i, c in enumerate(BASE_CONSONANTS)}\n",
        "\n",
        "DOUBLE_CONSONANTS = {\"ㄲ\": \"ㄱ\", \"ㄸ\": \"ㄷ\", \"ㅃ\": \"ㅂ\", \"ㅆ\": \"ㅅ\", \"ㅉ\": \"ㅈ\"}\n",
        "\n",
        "# 종성 복합 받침 분해 (기존과 동일 의미)\n",
        "CODA_COMPOUND = {\n",
        "    \"ㄳ\": (\"ㄱ\", \"ㅅ\"),\n",
        "    \"ㄵ\": (\"ㄴ\", \"ㅈ\"),\n",
        "    \"ㄶ\": (\"ㄴ\", \"ㅎ\"),\n",
        "    \"ㄺ\": (\"ㄹ\", \"ㄱ\"),\n",
        "    \"ㄻ\": (\"ㄹ\", \"ㅁ\"),\n",
        "    \"ㄼ\": (\"ㄹ\", \"ㅂ\"),\n",
        "    \"ㄽ\": (\"ㄹ\", \"ㅅ\"),\n",
        "    \"ㄾ\": (\"ㄹ\", \"ㅌ\"),\n",
        "    \"ㄿ\": (\"ㄹ\", \"ㅍ\"),\n",
        "    \"ㅀ\": (\"ㄹ\", \"ㅎ\"),\n",
        "    \"ㅄ\": (\"ㅂ\", \"ㅅ\"),\n",
        "}"
      ],
      "metadata": {
        "id": "hAoL0B8DnkPG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단모음 10개\n",
        "BASE_VOWELS = [\"ㅏ\",\"ㅓ\",\"ㅗ\",\"ㅜ\",\"ㅡ\",\"ㅣ\",\"ㅐ\",\"ㅔ\",\"ㅚ\",\"ㅟ\"]\n",
        "VOWEL2IDX = {v: i for i, v in enumerate(BASE_VOWELS)}\n",
        "\n",
        "# (기존 COMPOSED_VOWELS 활용: 야/여/요/유/ㅘ/ㅙ/ㅝ/ㅞ 등)\n",
        "COMPOSED_VOWELS = {\n",
        "    \"ㅑ\": (\"ㅣ\", \"ㅏ\"),\n",
        "    \"ㅒ\": (\"ㅣ\", \"ㅐ\"),\n",
        "    \"ㅕ\": (\"ㅣ\", \"ㅓ\"),\n",
        "    \"ㅖ\": (\"ㅣ\", \"ㅔ\"),\n",
        "    \"ㅛ\": (\"ㅣ\", \"ㅗ\"),\n",
        "    \"ㅠ\": (\"ㅣ\", \"ㅜ\"),\n",
        "\n",
        "    \"ㅘ\": (\"ㅗ\", \"ㅏ\"),\n",
        "    \"ㅙ\": (\"ㅗ\", \"ㅐ\"),\n",
        "    \"ㅝ\": (\"ㅜ\", \"ㅓ\"),\n",
        "    \"ㅞ\": (\"ㅜ\", \"ㅔ\"),\n",
        "    \"ㅢ\": (\"ㅡ\", \"ㅣ\")\n",
        "}"
      ],
      "metadata": {
        "id": "d5QW3Yg_oIj1"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ONSETS  = list(\"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\")\n",
        "NUCLEI  = list(\"ㅏㅐㅑㅒㅓㅔㅕㅖㅗㅘㅙㅚㅛㅜㅝㅞㅟㅠㅡㅢㅣ\")\n",
        "CODAS   = [\"\"] + list(\"ㄱㄲㄳㄴㄵㄶㄷㄹㄺㄻㄼㄽㄾㄿㅀㅁㅂㅄㅅㅆㅇㅈㅊㅋㅌㅍㅎ\")\n",
        "\n",
        "def decompose(syllable):\n",
        "    code = ord(syllable) - 0xAC00\n",
        "    onset_idx = code // 588\n",
        "    nucleus_idx = (code % 588) // 28\n",
        "    coda_idx = code % 28\n",
        "\n",
        "    onset = ONSETS[onset_idx]\n",
        "    nucleus = NUCLEI[nucleus_idx]\n",
        "    coda = CODAS[coda_idx] if coda_idx != 0 else None\n",
        "\n",
        "    return onset, nucleus, coda"
      ],
      "metadata": {
        "id": "OgEFKF69p7jp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consonant_base_and_double(jamo):\n",
        "    \"\"\"\n",
        "    자음 jamo를 (base_consonant, is_double)로 변환.\n",
        "    예: 'ㄲ' -> ('ㄱ', 1), 'ㄷ' -> ('ㄷ', 0)\n",
        "    \"\"\"\n",
        "    if jamo in DOUBLE_CONSONANTS:\n",
        "        return DOUBLE_CONSONANTS[jamo], 1\n",
        "    return jamo, 0\n",
        "\n",
        "def get_onset_feature_15(onset_jamo):\n",
        "    \"\"\"\n",
        "    onset: 14개 기본자음 one-hot + double_flag 1 = 15D\n",
        "    \"\"\"\n",
        "    base, is_double = consonant_base_and_double(onset_jamo)\n",
        "    feat = np.zeros(15, dtype=np.float32)\n",
        "    if base not in CONSONANT2IDX:\n",
        "        raise ValueError(f\"Unknown onset consonant: {onset_jamo}\")\n",
        "    feat[CONSONANT2IDX[base]] = 1.0\n",
        "    feat[14] = float(is_double)   # 마지막 차원: 쌍자음 토글\n",
        "    return feat\n",
        "\n",
        "def get_coda_feature_16(coda_jamo):\n",
        "    \"\"\"\n",
        "    coda: 14개 기본자음 few-hot + double_flag + no_coda_flag = 16D\n",
        "    - 복합 받침(ㄳ 등)은 few-hot으로 두 자음 bit 모두 1\n",
        "    - coda가 None이면: no_coda_flag = 1, 나머지 0\n",
        "    \"\"\"\n",
        "    feat = np.zeros(16, dtype=np.float32)\n",
        "\n",
        "    # 마지막 차원: no_coda_flag\n",
        "    if coda_jamo is None:\n",
        "        feat[15] = 1.0\n",
        "        return feat\n",
        "\n",
        "    # 복합 받침 처리\n",
        "    def apply_single_coda(j):\n",
        "        base, is_double = consonant_base_and_double(j)\n",
        "        if base not in CONSONANT2IDX:\n",
        "            raise ValueError(f\"Unknown coda consonant: {j}\")\n",
        "        feat[CONSONANT2IDX[base]] = 1.0\n",
        "        # double flag: 14번째 인덱스\n",
        "        if is_double:\n",
        "            feat[14] = 1.0\n",
        "\n",
        "    if coda_jamo in CODA_COMPOUND:\n",
        "        a, b = CODA_COMPOUND[coda_jamo]\n",
        "        apply_single_coda(a)\n",
        "        apply_single_coda(b)\n",
        "    else:\n",
        "        apply_single_coda(coda_jamo)\n",
        "\n",
        "    # no_coda_flag는 0 (이미 default)\n",
        "    return feat\n"
      ],
      "metadata": {
        "id": "9_r2472ARQTi"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _vowel_fewhot_single(jamo):\n",
        "    \"\"\"\n",
        "    단일 모음 jamo에 대한 10D one-hot.\n",
        "    (BASE_VOWELS에 없는 경우 에러)\n",
        "    \"\"\"\n",
        "    if jamo not in VOWEL2IDX:\n",
        "        raise ValueError(f\"Unknown base vowel: {jamo}\")\n",
        "    v = np.zeros(10, dtype=np.float32)\n",
        "    v[VOWEL2IDX[jamo]] = 1.0\n",
        "    return v\n",
        "\n",
        "def get_vowel_feature_10(nucleus_jamo):\n",
        "    \"\"\"\n",
        "    nucleus: 10D few-hot\n",
        "    - BASE_VOWELS에 있으면 one-hot\n",
        "    - COMPOSED_VOWELS에 있으면 구성요소 둘의 벡터를 OR (few-hot)\n",
        "    \"\"\"\n",
        "    # 단모음으로 바로 있는 경우\n",
        "    if nucleus_jamo in VOWEL2IDX:\n",
        "        return _vowel_fewhot_single(nucleus_jamo)\n",
        "\n",
        "    # 이중/복합 모음\n",
        "    if nucleus_jamo in COMPOSED_VOWELS:\n",
        "        a, b = COMPOSED_VOWELS[nucleus_jamo]\n",
        "        va = get_vowel_feature_10(a)\n",
        "        vb = get_vowel_feature_10(b)\n",
        "        # few-hot: 두 벡터를 OR (0/1)\n",
        "        v = va + vb\n",
        "        v = np.clip(v, 0.0, 1.0)\n",
        "        return v\n",
        "\n",
        "    # 그 밖의 모음은 필요에 따라 매핑 추가 가능\n",
        "    raise ValueError(f\"Unknown nucleus vowel: {nucleus_jamo}\")"
      ],
      "metadata": {
        "id": "TGRgc9NGRfYk"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_syllable_feature_41(syllable):\n",
        "    onset, nucleus, coda = decompose(syllable)\n",
        "\n",
        "    onset_feat  = get_onset_feature_15(onset)          # 15D\n",
        "    nucleus_feat = get_vowel_feature_10(nucleus)       # 10D\n",
        "    coda_feat   = get_coda_feature_16(coda)            # 16D\n",
        "\n",
        "    return np.concatenate([onset_feat, nucleus_feat, coda_feat])  # 41D\n",
        "\n",
        "\n",
        "char_embed = np.zeros((len(pron2id), 41), dtype=np.float32) # (11568, 41)\n",
        "print(char_embed.shape)\n",
        "\n",
        "for pron, idx in pron2id.items():\n",
        "    if len(pron) != 1:\n",
        "        continue\n",
        "    if ord(pron) < ord('가'):\n",
        "        continue\n",
        "    if ord('힣') < ord(pron):\n",
        "        continue\n",
        "    char_embed[idx] = get_syllable_feature_41(pron)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYbfOlceqALq",
        "outputId": "d4294d34-3473-4218-c0ac-8ca560339cee"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11568, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "char_embed = torch.from_numpy(char_embed)"
      ],
      "metadata": {
        "id": "sjMlNCXOM-Za"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NewEmb:\n",
        "```\n",
        "char  ->  OldEmb  ->  emb\n",
        "    + 41dim -> 128dim +\n",
        "```\n",
        "\n",
        "41dim: 발음정보 고정 emb\n",
        "128dim: 학습가능, Dense layer"
      ],
      "metadata": {
        "id": "5pGASGcttYTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ElectraEmbeddingWithNew(nn.Module):\n",
        "    def __init__(self, electra_embeddings, electra_embeddings_project, char_embed, adapt):\n",
        "        super().__init__()\n",
        "        self.old = electra_embeddings                  # 기존 ElectraEmbeddings\n",
        "        self.proj = electra_embeddings_project\n",
        "        self.pron_dim = char_embed.size(1)\n",
        "\n",
        "        # new embedding (11568X41), 학습하지 않음\n",
        "        self.register_buffer(\"char_embed\", char_embed, persistent=True)\n",
        "\n",
        "        # new embedding → 256 projection\n",
        "        self.new_up = nn.Linear(self.pron_dim, 256)\n",
        "        nn.init.zeros_(self.new_up.weight)\n",
        "        nn.init.zeros_(self.new_up.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        inputs_embeds=None,\n",
        "        past_key_values_length=0,\n",
        "    ):\n",
        "      # HF ElectraEmbeddings의 원래 forward와 동일한 시그니처로 맞추고,\n",
        "      # 내부에서 self.old(...) 를 그대로 호출하는 방식이 더 안전함.\n",
        "\n",
        "        # ---- Electra projection ----\n",
        "        old_emb_128 = self.old(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        old_emb_256 = self.proj(old_emb_128)\n",
        "\n",
        "        # ---- new embedding 41 차원 ----\n",
        "        new_emb_41 = self.char_embed[input_ids]     # (B, L, 41)\n",
        "        new_emb_256 = self.new_up(new_emb_41)\n",
        "\n",
        "        # ---- 256 차원에서 add ----\n",
        "        if adapt == False:\n",
        "            return old_emb_256, new_emb_256\n",
        "        final_emb = old_emb_256 + new_emb_256\n",
        "        return final_emb, new_emb_256\n"
      ],
      "metadata": {
        "id": "5tCtwIKfeyA1"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concat wrapper 모듈 적용\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5xhGm5XXVh_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectraWithCharEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    - peft_model.base_model.electra 를 backbone으로 사용\n",
        "    - electra.embeddings 를 ElectraEmbeddingWithNew 로 교체\n",
        "    - forward에서는 encoder 통과 후:\n",
        "      - sequence_output (encoder output)\n",
        "      - embedding_output (입력 임베딩)\n",
        "      을 함께 반환\n",
        "    \"\"\"\n",
        "    def __init__(self, peft_model, char_embed, adapt=True):\n",
        "        super().__init__()\n",
        "        self.peft_model = peft_model              # PeftModelForTokenClassification\n",
        "\n",
        "        # ElectraModel (LoRA 포함)\n",
        "        electra = peft_model.base_model.electra\n",
        "\n",
        "        # electra emb 교체\n",
        "        new_emb = ElectraEmbeddingWithNew(\n",
        "            electra.embeddings,\n",
        "            electra.embeddings_project,\n",
        "            char_embed,\n",
        "            adapt=adapt\n",
        "        )\n",
        "        electra.embeddings = new_emb\n",
        "        electra.embeddings_project = None\n",
        "\n",
        "        # char_embed를 바깥에서 접근할 수 있게 노출\n",
        "        self.char_embed = new_emb.char_embed      # (vocab_size, 41)\n",
        "        self.config = peft_model.base_model.config\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # 0) define electra\n",
        "        electra = self.peft_model.base_model.electra\n",
        "\n",
        "        # 1) input embedding\n",
        "        embedding_output, new_embedding_output = electra.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )  # (B, L, embed_dim=hidden_size)\n",
        "\n",
        "        # 2) attention mask / head mask\n",
        "        extended_mask = electra.get_extended_attention_mask(\n",
        "            attention_mask,\n",
        "            input_shape=input_ids.shape,\n",
        "            device=input_ids.device,\n",
        "        )\n",
        "\n",
        "        head_mask = electra.get_head_mask(\n",
        "            None, electra.config.num_hidden_layers\n",
        "        )\n",
        "\n",
        "        # 3) encoder 출력 (LoRA 적용됨)\n",
        "        encoder_outputs = electra.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_mask,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=electra.config.output_attentions,\n",
        "            output_hidden_states=electra.config.output_hidden_states,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs.last_hidden_state  # (B, L, hidden_size)\n",
        "\n",
        "        # 필요하면 encoder_outputs도 같이 넘길 수 있음\n",
        "        return {\n",
        "            \"sequence_output\": sequence_output,\n",
        "            \"embedding_output\": new_embedding_output, # new emb(only pron info)\n",
        "            # \"encoder_outputs\": encoder_outputs,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "IL9E_IDh8kx1"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PronunciationRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    한 음절에 대해:\n",
        "      - 입력: seq_state (N, 256), ctx_vec (N, 192 = 3*64)\n",
        "      - 출력:\n",
        "        - new_seq_state: (N, 256)   (tanh 된 상태)\n",
        "        - emb_pron:      (N, 41)    (activation 없음)\n",
        "        - is_end:        (N, 1)     (sigmoid)\n",
        "    여기서 N = B * L (배치 내 전체 토큰 수)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size_rnn: int = 256, ctx_dim: int = 192, out_embed_dim: int = 41):\n",
        "        super().__init__()\n",
        "        self.hidden_size_rnn = hidden_size_rnn   # 256\n",
        "        self.ctx_dim = ctx_dim                  # 192\n",
        "        self.out_embed_dim = out_embed_dim      # 41\n",
        "\n",
        "        # 1개의 fc로 256 + 41 + 1차원 한 번에 뽑기\n",
        "        self.fc = nn.Linear(hidden_size_rnn + ctx_dim,\n",
        "                            hidden_size_rnn + out_embed_dim + 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, seq_state, ctx_vec):\n",
        "        \"\"\"\n",
        "        seq_state: (N, 256)\n",
        "        ctx_vec:   (N, 192)\n",
        "        \"\"\"\n",
        "        x = torch.cat([seq_state, ctx_vec], dim=-1)  # (N, 256+192=448)\n",
        "        out = self.fc(x)                             # (N, 256+41+1=298)\n",
        "\n",
        "        h_part    = out[:, :self.hidden_size_rnn]                  # (N, 256)\n",
        "        emb_part  = out[:, self.hidden_size_rnn:self.hidden_size_rnn+self.out_embed_dim]  # (N, 41)\n",
        "        end_part  = out[:, -1:].contiguous()                       # (N, 1)\n",
        "\n",
        "        new_seq_state = torch.tanh(h_part)         # seq_state는 항상 tanh를 거친 형태\n",
        "        emb_pron      = emb_part                  # 발음 벡터는 activation 없음\n",
        "        is_end        = self.sigmoid(end_part)    # EOS 확률\n",
        "\n",
        "        return new_seq_state, emb_pron, is_end\n"
      ],
      "metadata": {
        "id": "GsSNLbKr-wch"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PronunciationTaggerRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ElectraWithCharEmbedding을 backbone으로 사용하는 발음 tagger.\n",
        "\n",
        "    - backbone:\n",
        "        input_ids, attention_mask, token_type_ids ->\n",
        "        sequence_output (B, L, 256)\n",
        "        embedding_output (B, L, 256)  # 여기서는 사용하지 않아도 됨\n",
        "\n",
        "    - 이 모델:\n",
        "        1) Electra hidden(256)을 23dim으로 축소 (act+fc)\n",
        "        2) 23dim + pron_feat(41dim)를 concat → 64dim syllable 벡터\n",
        "        3) prev/cur/next 64dim을 이어붙여 ctx_t (192dim) 생성\n",
        "           * prev = t-1 (mask면 t-2)\n",
        "           * next = t+1 (mask면 t+2)\n",
        "        4) ctx_t를 2-layer MLP(192→512→256, 마지막 tanh)를 통해 h0로 사용\n",
        "        5) RNNCell(h0, ctx_t) 한 번 호출 → emb_pron_t(41dim) 얻기\n",
        "        6) emb_pron_t와 char_embed[labels] 사이의 MSE loss 계산\n",
        "\n",
        "    - char_embed: (vocab_size, 41) few-hot 발음 feature\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone,              # ElectraWithCharEmbedding\n",
        "        num_labels: int,\n",
        "        ignore_index: int = -100,\n",
        "        space_mask_id: int = None,  # 필요하면 공백 토큰 id 지정 가능 (옵션)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.ignore_index = ignore_index\n",
        "        self.num_labels = num_labels\n",
        "        self.space_mask_id = space_mask_id\n",
        "\n",
        "        self.config = backbone.config\n",
        "        self.hidden_size_rnn = backbone.hidden_size     # 256\n",
        "\n",
        "        H_enc = self.hidden_size_rnn        # 256\n",
        "        H_pron = backbone.char_embed.size(1)  # 41\n",
        "        self.H_enc = H_enc\n",
        "        self.H_pron = H_pron\n",
        "\n",
        "        # 256 → 23 축소용\n",
        "        self.hidden_shrink = nn.Linear(H_enc, 23)\n",
        "        self.hidden_shrink_act = nn.GELU()\n",
        "\n",
        "        # syllable vec: 23 + 41 = 64\n",
        "        self.syll_dim = 64\n",
        "\n",
        "        # h0 초기화용 2-layer MLP: 192(3*64) → 512 → 256\n",
        "        self.h0_fc1 = nn.Linear(3 * self.syll_dim, 512)\n",
        "        self.h0_act1 = nn.GELU()\n",
        "        self.h0_fc2 = nn.Linear(512, H_enc)    # 256\n",
        "        # 마지막은 tanh로 state 안정화\n",
        "        self.h0_act2 = nn.Tanh()\n",
        "\n",
        "        # RNN cell: 256 state, 192 context, 41 output embed\n",
        "        self.cell = PronunciationRNNCell(\n",
        "            hidden_size_rnn=H_enc,\n",
        "            ctx_dim=3 * self.syll_dim,\n",
        "            out_embed_dim=H_pron,\n",
        "        )\n",
        "\n",
        "        # char_embed (vocab_size, 41)\n",
        "        self.char_embed = backbone.char_embed\n",
        "\n",
        "    def _build_mask_for_ctx(self, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        ctx용 mask 정의:\n",
        "        - 발음에 쓰지 않는 위치: CLS/SEP/공백 등\n",
        "        - pad는 attention_mask == 0으로 따로 빠진다고 가정\n",
        "        여기서는 기본적으로:\n",
        "            mask_ctx = (labels == ignore_index) & (attention_mask == 1)\n",
        "        로 두고, 필요 시 space_mask_id로 더 엄격히 잡을 수 있음.\n",
        "        \"\"\"\n",
        "        mask_ctx = (attention_mask == 1)\n",
        "\n",
        "        if labels is not None:\n",
        "            mask_ctx = mask_ctx & (labels == self.ignore_index)\n",
        "\n",
        "        # space_mask_id가 주어졌다면, 해당 토큰도 명시적으로 mask에 포함\n",
        "        if self.space_mask_id is not None:\n",
        "            space_positions = (input_ids == self.space_mask_id)\n",
        "            mask_ctx = mask_ctx | space_positions\n",
        "\n",
        "        return mask_ctx  # True = mask 위치 (발음에 안 쓰는 위치)\n",
        "\n",
        "    def _build_prev_next_idx(self, mask_ctx, valid_mask):\n",
        "        \"\"\"\n",
        "        pad를 제외하면 mask_ctx가 두 번 연속 나오지 않는다는 가정 하에,\n",
        "        prev index를:\n",
        "            prev1 = t-1\n",
        "            prev2 = t-2\n",
        "            prev = prev1 if prev1 not mask else prev2\n",
        "        로 정의하고, next도 대칭으로 정의.\n",
        "\n",
        "        valid_mask는 attention_mask == 1 같은 것 (pad 제외).\n",
        "        \"\"\"\n",
        "        B, L = mask_ctx.shape\n",
        "        device = mask_ctx.device\n",
        "\n",
        "        # 기본 index\n",
        "        idx = torch.arange(L, device=device)\n",
        "\n",
        "        # --- prev ---\n",
        "        prev1 = (idx - 1).clamp(min=0)      # [0,0,1,2,...]\n",
        "        prev2 = (idx - 2).clamp(min=0)      # [0,0,0,1,...]\n",
        "        prev1 = prev1.unsqueeze(0).expand(B, -1)  # (B, L)\n",
        "        prev2 = prev2.unsqueeze(0).expand(B, -1)\n",
        "\n",
        "        prev1_is_mask = mask_ctx.gather(1, prev1)  # (B, L)\n",
        "        prev_idx = torch.where(prev1_is_mask, prev2, prev1)  # (B, L)\n",
        "\n",
        "        # pad 위치는 prev 자체를 자기 자신으로 두거나 0으로 둬도 상관없음\n",
        "        # 여기서는 그냥 0으로 설정\n",
        "        prev_idx = prev_idx * valid_mask.long()\n",
        "\n",
        "        # --- next ---\n",
        "        next1 = (idx + 1).clamp(max=L - 1)\n",
        "        next2 = (idx + 2).clamp(max=L - 1)\n",
        "        next1 = next1.unsqueeze(0).expand(B, -1)\n",
        "        next2 = next2.unsqueeze(0).expand(B, -1)\n",
        "\n",
        "        next1_is_mask = mask_ctx.gather(1, next1)\n",
        "        next_idx = torch.where(next1_is_mask, next2, next1)\n",
        "\n",
        "        next_idx = next_idx * valid_mask.long()\n",
        "\n",
        "        return prev_idx, next_idx\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # 1) backbone 출력 (ElectraWithCharEmbedding)\n",
        "        backbone_outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            **kwargs,\n",
        "        )\n",
        "        seq_out = backbone_outputs[\"sequence_output\"]   # (B, L, 256)\n",
        "        B, L, H_enc = seq_out.shape\n",
        "\n",
        "        # 2) 발음 feature: char_embed에서 41dim 가져오기\n",
        "        #    (B, L) -> (B, L, 41)\n",
        "        char_embed = self.char_embed.to(input_ids.device)\n",
        "        pron_feat = char_embed[input_ids]          # (B, L, 41)\n",
        "\n",
        "        # 3) hidden 256 → 23 → syllable 64 = [23; 41]\n",
        "        hidden_23 = self.hidden_shrink_act(self.hidden_shrink(seq_out))  # (B, L, 23)\n",
        "        syll_vec = torch.cat([hidden_23, pron_feat], dim=-1)             # (B, L, 64)\n",
        "\n",
        "        # 4) ctx용 mask/valid_mask 만들기\n",
        "        #    - valid_mask: pad 제외 (attention_mask == 1)\n",
        "        #    - mask_ctx: 발음에 안 쓰는 위치 (CLS/SEP/공백 등)\n",
        "        valid_mask = (attention_mask == 1)          # (B, L) bool\n",
        "        mask_ctx = self._build_mask_for_ctx(input_ids, attention_mask, labels)\n",
        "\n",
        "        # 5) prev/next index (pad 제외, mask는 prevprev 규칙 사용)\n",
        "        prev_idx, next_idx = self._build_prev_next_idx(mask_ctx, valid_mask)\n",
        "\n",
        "        # 6) prev/cur/next 64dim 가져오기\n",
        "        D = self.syll_dim  # 64\n",
        "        # gather를 위해 index를 (B, L, 1) → (B, L, D)로 확장\n",
        "        gather_prev = prev_idx.unsqueeze(-1).expand(-1, -1, D)\n",
        "        gather_next = next_idx.unsqueeze(-1).expand(-1, -1, D)\n",
        "\n",
        "        syll_prev = syll_vec.gather(1, gather_prev)   # (B, L, 64)\n",
        "        syll_next = syll_vec.gather(1, gather_next)   # (B, L, 64)\n",
        "\n",
        "        # mask 위치 자체에서는 prev/next를 0으로\n",
        "        syll_prev = syll_prev.masked_fill(mask_ctx.unsqueeze(-1), 0.0)\n",
        "        syll_next = syll_next.masked_fill(mask_ctx.unsqueeze(-1), 0.0)\n",
        "\n",
        "        # 7) ctx_t = [prev; cur; next]  → (B, L, 192)\n",
        "        ctx = torch.cat([syll_prev, syll_vec, syll_next], dim=-1)  # (B, L, 192)\n",
        "\n",
        "        # 8) h0 초기화: ctx_t → 512 → 256 → tanh\n",
        "        ctx_flat = ctx.view(B * L, -1)          # (N, 192)\n",
        "        h0 = self.h0_act1(self.h0_fc1(ctx_flat))  # (N, 512)\n",
        "        h0 = self.h0_act2(self.h0_fc2(h0))        # (N, 256), tanh\n",
        "\n",
        "        # pad 위치는 state를 0으로 만들어도 됨\n",
        "        valid_flat = valid_mask.view(B * L)\n",
        "        h0 = h0 * valid_flat.unsqueeze(-1)       # (N, 256)\n",
        "\n",
        "        # 9) RNN cell 한 번 (현재 설계에서는 step=1)\n",
        "        seq_state, emb_pron_flat, is_end = self.cell(h0, ctx_flat)  # (N,256), (N,41), (N,1)\n",
        "\n",
        "        # 10) (B, L, 41)로 reshape\n",
        "        pred_embed = emb_pron_flat.view(B, L, self.H_pron)  # (B, L, 41)\n",
        "\n",
        "        # ===========================================================\n",
        "        # 11) Loss 계산 (MSE)\n",
        "        # ===========================================================\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # labels: (B, L), ignore_index = -100\n",
        "            with torch.no_grad():\n",
        "                labels_clamped = labels.clone()\n",
        "                labels_clamped = labels_clamped.masked_fill(\n",
        "                    labels_clamped == self.ignore_index, 0\n",
        "                )\n",
        "                char_embed = self.char_embed.to(labels_clamped.device)\n",
        "                labels_clamped = labels_clamped.to(char_embed.device)\n",
        "\n",
        "                target_embed = char_embed[labels_clamped]  # (B, L, 41)\n",
        "\n",
        "            mse = (pred_embed - target_embed) ** 2         # (B, L, 41)\n",
        "            mask = (labels != self.ignore_index).unsqueeze(-1)  # (B, L, 1)\n",
        "            mse = mse * mask\n",
        "\n",
        "            denom = mask.sum().clamp(min=1)   # 유효 토큰 수\n",
        "            loss = mse.sum() / denom\n",
        "\n",
        "        # ===========================================================\n",
        "        # 12) 최종 return\n",
        "        # ===========================================================\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": pred_embed,      # (B, L, 41)\n",
        "            # \"is_end\": is_end.view(B, L, 1)  # 필요하면 꺼내서 쓰면 됨\n",
        "        }\n",
        "\n",
        "    def predict_chars(self, logits):\n",
        "        \"\"\"\n",
        "        logits: (B, L, 41) - 예측된 발음 feature 벡터\n",
        "        return: pred_ids (B, L)  - 각 위치별로 '가','나' 같은 발음 vocab index\n",
        "\n",
        "        char_embed (V, 41)과 dot-product 기반 nearest neighbor.\n",
        "        \"\"\"\n",
        "        B, L, D = logits.shape\n",
        "        pred_flat = logits.reshape(B * L, D)             # (N, 41)\n",
        "\n",
        "        char_embed = self.char_embed.to(pred_flat.device)   # (V, 41)\n",
        "\n",
        "        sims = pred_flat @ char_embed.T                 # (N, V)\n",
        "        pred_ids_flat = sims.argmax(dim=-1)             # (N,)\n",
        "        pred_ids = pred_ids_flat.view(B, L)             # (B, L)\n",
        "\n",
        "        return pred_ids\n"
      ],
      "metadata": {
        "id": "DpObJnaO-1IM"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 진행"
      ],
      "metadata": {
        "id": "cuXEdLHUXRKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "RUpCGHRYZzu7"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits_torch = torch.tensor(logits)\n",
        "    predictions_torch = pron_model_rnn.predict_chars(logits_torch)\n",
        "    predictions = predictions_torch.detach().cpu().numpy()\n",
        "\n",
        "    # ignore_index = -100 제거 후 accuracy 계산\n",
        "    mask = labels != -100\n",
        "    y_true = labels[mask]\n",
        "    y_pred = predictions[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return {\"accuracy\": 0.0}\n",
        "\n",
        "    result = accuracy_metric.compute(predictions=y_pred, references=y_true)\n",
        "    return {\"accuracy\": result[\"accuracy\"]}\n"
      ],
      "metadata": {
        "id": "HbhZrK-ZZ0cg"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "num_labels = len(pron2id)\n",
        "\n",
        "base_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        ")\n"
      ],
      "metadata": {
        "id": "iPrWeE7UZv5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a105e5-ac7a-46eb-de17-2b069c0ef1d6"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/kocharelectra-small-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"]  # Electra의 attention/FFN 모듈 이름 기준\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(base_model, lora_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "EfF1Z-tBZxt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da4bf0e-c0c1-46a1-82ef-de97bc824305",
        "collapsed": true
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,415,344 || all params: 17,445,216 || trainable%: 19.5775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) backbone 생성\n",
        "backbone = ElectraWithCharEmbedding(peft_model=model, char_embed=char_embed, adapt=False)\n",
        "\n",
        "# 2) RNN 기반 tagger\n",
        "pron_model_rnn = PronunciationTaggerRNN(\n",
        "    backbone=backbone,\n",
        "    num_labels=num_labels,\\\n",
        "    ignore_index=-100,\n",
        ")"
      ],
      "metadata": {
        "id": "5g_kQZ8j9GYA"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"kocharelectra-pron-lora\",\n",
        "    learning_rate=1e-3,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=100,\n",
        "    fp16=False,          # GPU가 지원하면 속도↑\n",
        "    bf16=False,\n",
        "    report_to=\"none\",   # wandb 등 안 쓸 거면 none\n",
        "    eval_accumulation_steps=16,\n",
        "    # prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=pron_model_rnn,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "X4C4U3aWZ3B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "uVPgEk5gZ4Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(save_dir)   # ✅ 모델 가중치 저장 (pytorch_model.bin)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# 발음 vocab도 같이 저장\n",
        "with open(os.path.join(save_dir, \"pron_vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pron2id, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "9t_kFopbZ6cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "mI4zM3h1GQGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# 1) pron_vocab 로드\n",
        "with open(os.path.join(save_dir, \"pron_vocab.json\"), encoding=\"utf-8\") as f:\n",
        "    pron2id = json.load(f)\n",
        "id2pron = {v: k for k, v in pron2id.items()}\n",
        "\n",
        "# 3) safetensors 로드\n",
        "state = load_file(os.path.join(save_dir, \"model.safetensors\"))\n",
        "pron_model_rnn.load_state_dict(state, strict=False)\n",
        "pron_model_rnn.to('cuda')\n",
        "\n",
        "# 4) tokenizer 로드\n",
        "tokenizer = KoCharElectraTokenizer.from_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "MogW9hduVolI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_tokenized,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "device = next(pron_model_rnn.parameters()).device\n",
        "pron_model_rnn.eval()\n",
        "\n",
        "all_pred_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        output = pron_model_rnn.forward(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch.get(\"token_type_ids\", None),\n",
        "        )  # (B, L)\n",
        "        pred_ids = pron_model_rnn.predict_chars(output[\"logits\"])\n",
        "\n",
        "        # GPU → CPU → numpy\n",
        "        pred_ids = pred_ids.cpu().numpy()\n",
        "        masks = batch[\"attention_mask\"].cpu().numpy()\n",
        "        input_ids = batch[\"input_ids\"].cpu()\n",
        "        labels = batch[\"labels\"].cpu()\n",
        "\n",
        "        texts = tokenizer.batch_decode(\n",
        "            input_ids,\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        B = input_ids.size(0)\n",
        "\n",
        "        for i in range(B):\n",
        "            seq_ids = pred_ids[i]\n",
        "            mask = masks[i]\n",
        "            text = texts[i]\n",
        "            label = labels[i]\n",
        "            valid = (label != -100)\n",
        "\n",
        "            pred = [\n",
        "                id2pron[int(idx)]\n",
        "                for idx, m in zip(seq_ids.tolist(), valid.tolist())\n",
        "                if m\n",
        "            ]\n",
        "\n",
        "            gold = []\n",
        "            for idx, m in zip(label.tolist(), valid.tolist()):\n",
        "                if not m:\n",
        "                    continue\n",
        "                if idx == -100:\n",
        "                    continue\n",
        "                gold.append(id2pron[int(idx)])\n",
        "\n",
        "            print(\"TEXT:\", text)\n",
        "            print(\"PRED:\", \"\".join(pred))\n",
        "            print(\"GOLD:\", \"\".join(gold))\n",
        "            print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "PnUvikguGP8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dummy model 평가"
      ],
      "metadata": {
        "id": "TIEDYZO0bBL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTextAsPronModel(nn.Module):\n",
        "    \"\"\"\n",
        "    input_ids에서 각 토큰의 char_embed를 그대로 logits으로 내보내는 더미 모델.\n",
        "    => predict_chars(logits)를 돌리면 '입력 텍스트 그대로'가 예측 발음이 됨.\n",
        "    \"\"\"\n",
        "    def __init__(self, char_embed: torch.Tensor):\n",
        "        super().__init__()\n",
        "        # char_embed: (vocab_size, H_pron)\n",
        "        self.register_buffer(\"char_embed\", char_embed)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        input_ids: (B, L)\n",
        "        labels: (B, L), ignore_index=-100\n",
        "        \"\"\"\n",
        "        # (B, L, H_pron) : 각 위치 토큰 id -> 그 토큰의 발음 feature\n",
        "        logits = self.char_embed[input_ids]\n",
        "\n",
        "        # loss는 dummy (학습 안 할 거라 상관 없음)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "03QjC3T-axV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_model = DummyTextAsPronModel(char_embed=char_embed).to(\"cuda\")\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "dummy_trainer = Trainer(\n",
        "    model=dummy_model,\n",
        "    args=training_args,      # 기존 training_args 재사용해도 됨 (train 안 할 거면 epoch 이런 건 무시)\n",
        "    train_dataset=train_tokenized,   # 안 써도 되지만 형식 맞춰둠\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# baseline: \"철자 그대로 발음\"의 accuracy\n",
        "dummy_results = dummy_trainer.evaluate()\n",
        "print(dummy_results)\n"
      ],
      "metadata": {
        "id": "h9AopyYybATG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()\n"
      ],
      "metadata": {
        "id": "4NUq-icot7Gk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}